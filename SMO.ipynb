{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SMO.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0yqomvFmwAKD",
        "colab_type": "text"
      },
      "source": [
        "### Problème sous-jacent à un Support Vector Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gm76foR_v5ZE",
        "colab_type": "text"
      },
      "source": [
        "Le SVM est un classifier binaire pouvant être étende à la classification multiclasse dont l'objectif\n",
        "est de maximiser la marge entre les deux classes. Une pénalité est attribué aux points se trouvant\n",
        "du mauvais côté de la marge. Il s'agit d'un problème d'optimisation quadratique qui consiste à minimiser\n",
        "la fonction suivante:\n",
        "\n",
        "$L = \\frac{\\|w\\|^2}{2} - \\displaystyle\\sum \\alpha_i y_i (\\mathbf x_i \\mathbf w + b)$\n",
        "\n",
        "En dérivant selon $\\mathbf{w}$ et $\\mathbf{b}$, on peut réécrire le problème sous sa forme dual qui revient à maximiser:\n",
        "\n",
        "$L = \\displaystyle\\sum \\alpha_i - \\frac{1}{2}\\displaystyle\\sum \\alpha_i \\alpha_j y_i y_j (\\mathbf x_i \\mathbf x_j)$\n",
        "\n",
        "On voit que le pronlème qui nous est posé dépend du produit scalaire entre deux échantillons. La règle de\n",
        "décision s'écrit sous la forme suivante:\n",
        "\n",
        "$D = \\displaystyle\\sum \\alpha_i y_i (\\mathbf x_i \\mathbf u) + b >= 0 $ (u vecteur inconnu)\n",
        "\n",
        "Dans le cas où le problème n'est pas linéairement séparable on peut calculer ce produit scalaire dans un autre espace\n",
        "qui nous donne de meilleurs perspectives, avec comme avantage le fait de ne pas avoir à effectuer la transformation dans\n",
        "le nouvel espace. Cela s'appelle le kernel trick. Par la suite nous étudierons l'impact du choix du noyau\n",
        "quant à la performance du SVM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aK5xapMlnr5T",
        "colab_type": "text"
      },
      "source": [
        "### Simple SMO Implementation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deQdbFvQps1w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8PZBYPnfnwdy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SVM():\n",
        "  def __init__(self, C=1.0, tol=0.01, max_passes=10, kernel='linear'):\n",
        "    self.C = C\n",
        "    self.tol = tol\n",
        "    self.max_passes = max_passes\n",
        "    self.kernel = kernel\n",
        "    self.kernels = { 'linear' : self.kernel_linear,\n",
        "                     'polynomial' : self.kernel_polynomial,\n",
        "                     'rbf' : self.kernel_rbf,\n",
        "                     'sigmoid' : self.kernel_sigmoid,\n",
        "                   }\n",
        "    self.alphas = []\n",
        "    self.b = 0\n",
        "    self.x = []\n",
        "    self.y = []\n",
        "    self.m = 0\n",
        "  \n",
        "  def predict(self, x_pred):\n",
        "    K = self.kernels[self.kernel]\n",
        "    out = 0\n",
        "    for i in range(self.m):\n",
        "      out += self.alphas[i] * self.y[i] * K(self.x[i], x_pred) \\\n",
        "             + self.b\n",
        "    return np.sign(out)\n",
        "                   \n",
        "  def fit(self, x_train, y_train):\n",
        "    self.x = np.copy(x_train)\n",
        "    self.y = np.copy(y_train)\n",
        "    self.m = x_train.shape[0]\n",
        "    passes = 0\n",
        "    self.alphas = np.zeros(self.m)\n",
        "    old_alphas = np.zeros(self.m)\n",
        "\n",
        "    while passes < self.max_passes:\n",
        "      num_changed_alphas = 0\n",
        "      \n",
        "      for i in range(self.m):\n",
        "        Ei = self.predict(self.x[i]) - self.y[i]\n",
        "        if ((self.y[i] * Ei < -self.tol) and (self.alphas[i] < self.C)) \\\n",
        "            or ((self.y[i] * Ei > self.tol) and (self.alphas[i] > 0)):\n",
        "          \n",
        "          j = self.get_random(i)\n",
        "          Ej = self.predict(self.x[j]) - self.y[j]\n",
        "          old_alphas[i], old_alphas[j] = self.alphas[i], self.alphas[j]\n",
        "          \n",
        "          L, H = self.compute_L_H(self.alphas[j], self.alphas[j],\n",
        "                                  self.y[i], self.y[j])        \n",
        "          if L == H:\n",
        "            continue\n",
        "            \n",
        "          n = self.compute_n(self.x[i], self.x[j])\n",
        "          if n >= 0:\n",
        "            continue\n",
        "            \n",
        "          self.alphas[j] = self.alphas[j] - float(self.y[j] * (Ei - Ej)) / n\n",
        "          self.alphas[j] = max(self.alphas[j], L)\n",
        "          self.alphas[j] = min(self.alphas[j], H)\n",
        "          \n",
        "          if abs(self.alphas[j] - old_alphas[j]) < 0.00001:\n",
        "            continue\n",
        "            \n",
        "          self.alphas[i] = self.alphas[i] + self.y[i] * self.y[j] \\\n",
        "                           * (old_alphas[j] - self.alphas[j])\n",
        "            \n",
        "          b1 = self.b - Ei - self.y[i] * (self.alphas[i] - old_alphas[i]) \\\n",
        "               * np.dot(self.x[i], self.x[i]) - self.y[j] \\\n",
        "               * (self.alphas[j] - old_alphas[j]) * np.dot(self.x[i], self.x[j])\n",
        "              \n",
        "          b2 = self.b - Ej - self.y[i] * (self.alphas[i] - old_alphas[i]) \\\n",
        "               * np.dot(self.x[i], self.x[j]) - self.y[j] \\\n",
        "               * (self.alphas[j] - old_alphas[j]) * np.dot(self.x[j], self.x[j])\n",
        "          \n",
        "          self.b = self.compute_b(b1, b2, self.alphas[i], self.alphas[j])\n",
        "          num_changed_alphas += 1\n",
        "\n",
        "      if num_changed_alphas == 0:\n",
        "        passes += 1\n",
        "      else:\n",
        "        passes = 0\n",
        "    return self.alphas, self.b\n",
        "          \n",
        "  def compute_L_H(self, ai, aj, yi, yj):\n",
        "    if(yi != yj):\n",
        "      return (max(0, aj - ai), min(self.C, self.C + aj - ai))\n",
        "    else:\n",
        "      return (max(0, ai + aj - self.C), min(self.C, ai + aj))\n",
        "  \n",
        "  def compute_n(self, xi, xj):\n",
        "    K = self.kernels[self.kernel]\n",
        "    return  2 * K(xi, xj) - K(xi, xi) - K(xj, xj)\n",
        "  \n",
        "  def compute_b(self, b1, b2, ai, aj):\n",
        "    if ai > 0 and ai < self.C:\n",
        "      return b1\n",
        "    if aj > 0 and aj < self.C:\n",
        "      return b2\n",
        "    return (b1 + b2) / 2\n",
        "  \n",
        "  def get_random(self, i):\n",
        "    j = i\n",
        "    while j == i:\n",
        "      j = random.randint(0, self.m - 1)\n",
        "    return j\n",
        "\n",
        "  def kernel_linear(self, x, y):\n",
        "    return np.dot(x, y.T)\n",
        "  \n",
        "  def kernel_polynomial(self, x, y, d=2):\n",
        "    return np.dot(x, y.T)**d\n",
        "  \n",
        "  def kernel_rbf(self, x, y, sigma=1.0):\n",
        "    return np.exp(-((np.linalg.norm(x - y)**2) / (2 * sigma**2)))\n",
        "  \n",
        "  def kernel_sigmoid(self, x, y):\n",
        "    return np.tanh(np.dot(x, y.T))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyKfdv3cnxHs",
        "colab_type": "text"
      },
      "source": [
        "### Load Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjqc4On7nzKM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjIBkQCbJCNR",
        "colab_type": "code",
        "outputId": "215c3aa0-5594-4549-b780-9ac823e267cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "train_filter = np.where((y_train == 3) | (y_train == 4))\n",
        "test_filter = np.where((y_test == 3) | (y_test == 4))\n",
        "x_train, y_train = x_train[train_filter], y_train[train_filter]\n",
        "x_test, y_test = x_test[test_filter], y_test[test_filter]\n",
        "image_test = np.copy(x_test)\n",
        "x_train.shape, x_test.shape"
      ],
      "execution_count": 316,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((11973, 28, 28), (1992, 28, 28))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 316
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7EPhj5-oKSz7",
        "colab_type": "code",
        "outputId": "8d8cf43c-b25c-4720-a076-90e85c4044cc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "y_train = y_train.astype(int)\n",
        "y_test = y_test.astype(int)\n",
        "y_train[y_train == 3] = 1\n",
        "y_train[y_train == 4] = -1\n",
        "y_test[y_test == 3] = 1\n",
        "y_test[y_test == 4] = -1\n",
        "print(y_train, y_test)"
      ],
      "execution_count": 317,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[-1  1 -1 ...  1  1  1] [-1 -1  1 ... -1  1 -1]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y6nLseb-Gj1r",
        "colab_type": "text"
      },
      "source": [
        "### Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRRTA2R5Glpf",
        "colab_type": "code",
        "outputId": "73f620bc-669b-460d-ec8a-fc7ff3fe6908",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "x_train = x_train.reshape(x_train.shape[0], 784)\n",
        "x_test = x_test.reshape(x_test.shape[0], 784)\n",
        "\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('Number of images in x_train', x_train.shape[0])\n",
        "print('Number of images in x_test', x_test.shape[0])"
      ],
      "execution_count": 318,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (11973, 784)\n",
            "Number of images in x_train 11973\n",
            "Number of images in x_test 1992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xJ9iFjZBvQ0",
        "colab_type": "text"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4seSvCTsZdec",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def accuracy(y, y_hat):\n",
        "    idx = np.where(y_hat == 1)\n",
        "    true_pos = np.sum(y_hat[idx] == y[idx])\n",
        "    idx = np.where(y_hat == -1)\n",
        "    true_neg = np.sum(y_hat[idx] == y[idx])\n",
        "    return float(true_pos + true_neg) / y.shape[0]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjizcEvFBwTm",
        "colab_type": "code",
        "outputId": "508c773d-79dd-40b0-945e-2ccd514730fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "kernels = ['linear', 'polynomial', 'rbf', 'sigmoid']\n",
        "\n",
        "for k in kernels:\n",
        "  model = SVM(kernel=k)\n",
        "  alphas, b = model.fit(x_train[:100], y_train[:100])\n",
        "  #print(alphas, b)\n",
        "\n",
        "  n = x_test.shape[0]\n",
        "  y_pred = np.zeros(n)\n",
        "\n",
        "  print(\"Running %s tests\" % str(n))\n",
        "  for i in range(n):\n",
        "    y_pred[i] = model.predict(x_test[i])\n",
        "    #if (y_test[i] != y_pred[i]):\n",
        "    #  print(y_test[i], y_pred[i])\n",
        "  acc = calc_acc(y_test, y_pred)\n",
        "  print(\"accuracy for kernel %s: %s\\n\" % (k, str(acc)))"
      ],
      "execution_count": 347,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Running 1992 tests\n",
            "accuracy for kernel linear: 0.9849397590361446\n",
            "\n",
            "Running 1992 tests\n",
            "accuracy for kernel polynomial: 0.9196787148594378\n",
            "\n",
            "Running 1992 tests\n",
            "accuracy for kernel rbf: 0.4929718875502008\n",
            "\n",
            "Running 1992 tests\n",
            "accuracy for kernel sigmoid: 0.4929718875502008\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B4fa1umMYOJ-",
        "colab_type": "text"
      },
      "source": [
        "### References\n",
        "\n",
        "[1] John C. Platt\n",
        "*Sequential Minimal Optimization: A Fast Algorithm for Training Support Vector Machines*,\n",
        "Microsoft Research 1998"
      ]
    }
  ]
}